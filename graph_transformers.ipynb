{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Transformer Model\n",
    "\n",
    "DiGress trained a graph transformer network proposed by Dwivedi & Bresson (2021) s the denoising model. In this section, we will go through the implementation details of the model to see how it predicts the clean graph from the noisy graph. \n",
    "\n",
    "\n",
    "- **$\\mathbf{X}$**: Node features matrix, shape ( bs, $n, d_x$ )\n",
    "- **$\\mathbf{E}$**: Edge features matrix, shape (bs, $n, n, d_e$ )\n",
    "- **$\\mathbf{y}$**: Global features vector, shape (bs, $d_y$ )\n",
    "- **node_mask**: Node mask, shape (bs, $n$)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules.linear import Linear\n",
    "from torch.nn.modules.normalization import LayerNorm\n",
    "from torch.nn.modules.dropout import Dropout\n",
    "from torch import Tensor\n",
    "import math\n",
    "from src.utils import PlaceHolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def assert_correctly_masked(variable, node_mask):\n",
    "    # Check that the masked elements are close to zero\n",
    "    assert (variable * (1 - node_mask.long())).abs().max().item() < 1e-4, \\\n",
    "        'Variables not masked properly.'\n",
    "\n",
    "def masked_softmax(x, mask, **kwargs):\n",
    "    if mask.sum() == 0:\n",
    "        return x\n",
    "    x_masked = x.clone()\n",
    "    # set elements where the mask is 0 to negative infinity\n",
    "    x_masked[mask == 0] = -float(\"inf\")\n",
    "    return torch.softmax(x_masked, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping Node/Edge Features($\\mathbf X, \\mathbf E$) to Global Features $y$\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\operatorname{PNA}(\\boldsymbol{X})=\\operatorname{cat}(\\max (\\boldsymbol{X}), \\min (\\boldsymbol{X}), \\operatorname{mean}(\\boldsymbol{X}), \\operatorname{std}(\\boldsymbol{X})) \\boldsymbol{W}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Xtoy(nn.Module):\n",
    "    def __init__(self, dx, dy):\n",
    "        \"\"\" Map node features to global features \"\"\"\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(4 * dx, dy)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\" X: bs, n, dx. \"\"\"\n",
    "        m = X.mean(dim=1)     # bs, dx\n",
    "        mi = X.min(dim=1)[0]  # bs, dx\n",
    "        ma = X.max(dim=1)[0]  # bs, dx\n",
    "        std = X.std(dim=1)    # bs, dx\n",
    "        z = torch.hstack((m, mi, ma, std)) # bs, 4 * dx\n",
    "        out = self.lin(z)    # bs, dy\n",
    "        return out\n",
    "\n",
    "\n",
    "class Etoy(nn.Module):\n",
    "    def __init__(self, d, dy):\n",
    "        \"\"\" Map edge features to global features. \"\"\"\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(4 * d, dy)\n",
    "\n",
    "    def forward(self, E):\n",
    "        \"\"\" E: bs, n, n, de\n",
    "            Features relative to the diagonal of E could potentially be added.\n",
    "        \"\"\"\n",
    "        m = E.mean(dim=(1, 2))              # bs, de\n",
    "        mi = E.min(dim=2)[0].min(dim=1)[0]  # bs, de\n",
    "        ma = E.max(dim=2)[0].max(dim=1)[0]  # bs, de\n",
    "        std = torch.std(E, dim=(1, 2))      # bs, de\n",
    "        z = torch.hstack((m, mi, ma, std))  # bs, 4 * de\n",
    "        out = self.lin(z)                   # bs, dy\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self Attention `NodeEdgeBlock`\n",
    "Self-attention layer that updates the representations on the node, edges, and global features\n",
    "$$\\mathbf{X}_{\\text {new }}, \\mathbf{E}_{\\text {new}}, \\mathbf{y}_{\\text {new}}=\\operatorname{SelfAttn}\\left(\\mathbf{X}, \\mathbf{E}, \\mathbf{y}, \\text{node}_\\text{mask}\\right)$$\n",
    "\n",
    "### 1. Linear Projections and Maskings\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "& \\mathbf{Q}=\\mathbf{X} \\mathbf{W}_Q \\odot \\mathbf{x}_{\\text {mask }} \\\\\n",
    "& \\mathbf{K}=\\mathbf{X} \\mathbf{W}_K \\odot \\mathbf{x}_{\\text {mask }} \\\\\n",
    "& \\mathbf{V}=\\mathbf{X} \\mathbf{W}_V \\odot \\mathbf{x}_{\\text {mask }}\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "### 2. Reshape $\\mathbf{Q,K,V}$\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "   & \\mathbf Q = \\mathbf Q. \\textrm{reshape(bs, n, nhead, df)} \\\\\n",
    "   & \\mathbf K = \\mathbf K. \\textrm{reshape(bs, n, nhead, df)} \\\\\n",
    "   & \\mathbf V = \\mathbf V. \\textrm{reshape(bs, n, nhead, df)}\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "### 2. Calculate the Attention Score \n",
    "The query, key, and value matrices are transformed into multi-head\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf Y = \\frac{(\\mathbf Q \\times \\mathbf K^T)}{\\sqrt{\\textrm{df}}}\n",
    "\\end{equation}\n",
    "\n",
    "### 3. FiLM Layers\n",
    "\n",
    "\n",
    "#### (a) Incorporate Edge Features to the self-attention score\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\mathbf{E_1} & =\\left( \\mathbf W^{\\textrm{emul}}_E \\mathbf{E} \\odot \\mathbf{e_{\\textrm{mask}}} ).\\textrm{reshape}(\\mathrm{bs}, \\mathrm{n}, \\mathrm{n}, \\mathrm{n} \\text { head, df })\\right. \\\\\n",
    "\\mathbf{E_2} & =\\left( \\mathbf W^{\\textrm{add}}_E \\mathbf{E} \\odot \\mathbf{e_{\\textrm{mask}}} ).\\textrm{reshape}(\\mathrm{bs}, \\mathrm{n}, \\mathrm{n}, \\mathrm{n} \\text { head, df })\\right. \\\\\n",
    "\\mathbf{Y} & =\\mathbf{Y} \\odot(\\mathbf{E} \\mathbf{1}+1)+\\mathbf{E} \\mathbf{2}\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Incorporate $y$ to $\\mathbf E$\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "&  E_{\\textrm{new}} = \\mathbf Y.\\textrm{reshape}(\\textrm{bs, n, n, dx}) \\\\\n",
    "& \\mathbf E_{\\textrm{out}} = W^{\\textrm{add}}_{ye}y +  (W^{\\textrm{mul}}_{ye}y +1 ) \\odot \\textrm{new}\\mathbf E \\odot \\mathbf{e_{\\textrm{mask}}}\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "### 4. Compute Normalized Attention Scores \n",
    "\n",
    "\\begin{equation}\n",
    "    \\textrm{Attn} = \\textrm{softmax} (\\mathbf Y \\odot \\textrm{softmax}_{\\textrm{mask}}) \\in \\mathbb R^{bs \\times n\\times n \\odot \\textrm{nhead}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Compute Weighted Values \n",
    "This step aggregates information from connected nodes weighted by the computed attention scores, effectively updating node representations based on their neighborhood.\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf V_{\\textrm{weighted}} = \\sum \\textrm{Attn} \\times \\mathbf V \\in \\mathbb R^{bs \\times n \\times dx}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Update Representations\n",
    "Node, edge, and global feature representations are updated through additional FiLM layers and linear transformations:\n",
    "\n",
    "#### (a) Incorporate $y$ to $\\mathbf X$\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "    &  \\mathbf X_{\\textrm{new}} = W_{yx}^{\\textrm{add}} + (W_{yx}^{\\textrm{mul}} + 1) \\odot \\mathbf V_{\\textrm{weighted}} \\\\\n",
    "    &  \\mathbf X_{\\textrm{out}} = \\mathbf X_{\\textrm{new}} W_{xx} \\odot \\mathbf x_{\\textrm{mask}}\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "#### (b) Process $y$ based on $\\mathbf{X, E}$\n",
    "\n",
    "\\begin{equation}\n",
    "    y_{\\textrm{out}} = (yW_{yy} + \\mathbf E W_{ey} + X W_{xy}) W_{yy}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeEdgeBlock(nn.Module):\n",
    "    def __init__(self, dx, de, dy, n_head, **kwargs):\n",
    "        super().__init__()\n",
    "        assert dx % n_head == 0, f\"dx: {dx} -- nhead: {n_head}\"\n",
    "        self.dx = dx\n",
    "        self.de = de\n",
    "        self.dy = dy\n",
    "        self.df = int(dx / n_head)\n",
    "        self.n_head = n_head\n",
    "\n",
    "        # Attention\n",
    "        self.q = Linear(dx, dx)\n",
    "        self.k = Linear(dx, dx)\n",
    "        self.v = Linear(dx, dx)\n",
    "        # FiLM E to X\n",
    "        self.e_add = Linear(de, dx)\n",
    "        self.e_mul = Linear(de, dx)\n",
    "        # FiLM y to E\n",
    "        self.y_e_mul = Linear(dy, dx)           # Warning: here it's dx and not de\n",
    "        self.y_e_add = Linear(dy, dx)\n",
    "        # FiLM y to X\n",
    "        self.y_x_mul = Linear(dy, dx)\n",
    "        self.y_x_add = Linear(dy, dx)\n",
    "        # Process y\n",
    "        self.y_y = Linear(dy, dy)\n",
    "        self.x_y = Xtoy(dx, dy)\n",
    "        self.e_y = Etoy(de, dy)\n",
    "        # Output layers\n",
    "        self.x_out = Linear(dx, dx)\n",
    "        self.e_out = Linear(dx, de)\n",
    "        self.y_out = nn.Sequential(nn.Linear(dy, dy), nn.ReLU(), nn.Linear(dy, dy))\n",
    "\n",
    "    def forward(self, X, E, y, node_mask):\n",
    "        \"\"\"\n",
    "        :param X: bs, n, d        node features\n",
    "        :param E: bs, n, n, d     edge features\n",
    "        :param y: bs, dy           global features\n",
    "        :param node_mask: bs, n\n",
    "        :return: newX, newE, new_y with the same shape.\n",
    "        \"\"\"\n",
    "        bs, n, _ = X.shape\n",
    "        x_mask = node_mask.unsqueeze(-1)        # bs, n, 1\n",
    "        e_mask1 = x_mask.unsqueeze(2)           # bs, n, 1, 1\n",
    "        e_mask2 = x_mask.unsqueeze(1)           # bs, 1, n, 1\n",
    "\n",
    "        # 1. Map X to keys and queries\n",
    "        Q = self.q(X) * x_mask                  # (bs, n, dx)\n",
    "        K = self.k(X) * x_mask           \n",
    "        V = self.v(X) * x_mask               \n",
    "        assert_correctly_masked(Q, x_mask)\n",
    "        \n",
    "        # 2. Reshape to (bs, n, n_head, df) with dx = n_head * df\n",
    "        Q = Q.reshape((Q.size(0), Q.size(1), self.n_head, self.df))\n",
    "        K = K.reshape((K.size(0), K.size(1), self.n_head, self.df))\n",
    "        V = V.reshape((V.size(0), V.size(1), self.n_head, self.df))\n",
    "\n",
    "        Q = Q.unsqueeze(2)                              # (bs, 1, n, n_head, df)\n",
    "        K = K.unsqueeze(1)                              # (bs, n, 1, n head, df)\n",
    "        V = V.unsqueeze(1)                              # (bs, 1, n, n_head, df)\n",
    "\n",
    "        # Compute unnormalized attentions. Y is (bs, n, n, n_head, df)\n",
    "        Y = Q * K\n",
    "        Y = Y / math.sqrt(Y.size(-1)) \n",
    "\n",
    "        assert_correctly_masked(Y, (e_mask1 * e_mask2).unsqueeze(-1))\n",
    "\n",
    "        E1 = self.e_mul(E) * e_mask1 * e_mask2                        # bs, n, n, dx\n",
    "        E1 = E1.reshape((E.size(0), E.size(1), E.size(2), self.n_head, self.df)) # bs, n, n, n_head, df\n",
    "\n",
    "\n",
    "        E2 = self.e_add(E) * e_mask1 * e_mask2                        # bs, n, n, dx\n",
    "        E2 = E2.reshape((E.size(0), E.size(1), E.size(2), self.n_head, self.df))\n",
    "\n",
    "        # Incorporate edge features to the self attention scores.\n",
    "        Y = Y * (E1 + 1) + E2                  # (bs, n, n, n_head, df)\n",
    "\n",
    "        # Incorporate y to E\n",
    "        newE = Y.flatten(start_dim=3)                    # bs, n, n, dx\n",
    "        ye1 = self.y_e_add(y).unsqueeze(1).unsqueeze(1)  # bs, 1, 1, de\n",
    "        ye2 = self.y_e_mul(y).unsqueeze(1).unsqueeze(1)\n",
    "        newE = ye1 + (ye2 + 1) * newE\n",
    "\n",
    "        # Output E\n",
    "        newE = self.e_out(newE) * e_mask1 * e_mask2      # bs, n, n, de\n",
    "        assert_correctly_masked(newE, e_mask1 * e_mask2)\n",
    "\n",
    "        # Compute attentions. attn is still (bs, n, n, n_head, df)\n",
    "        softmax_mask = e_mask2.expand(-1, n, -1, self.n_head)    # bs, 1, n, 1\n",
    "        attn = masked_softmax(Y, softmax_mask, dim=2)  # bs, n, n, n_head\n",
    "\n",
    "        # Compute weighted values\n",
    "        weighted_V = attn * V                          # bs, n, n, n_head, df\n",
    "        weighted_V = weighted_V.sum(dim=2)             # bs, n, n_head, df\n",
    "\n",
    "        # Send output to input dim\n",
    "        weighted_V = weighted_V.flatten(start_dim=2)            # bs, n, dx\n",
    "\n",
    "        # Incorporate y to X\n",
    "        yx1 = self.y_x_add(y).unsqueeze(1)\n",
    "        yx2 = self.y_x_mul(y).unsqueeze(1)\n",
    "        newX = yx1 + (yx2 + 1) * weighted_V\n",
    "\n",
    "        # Output X\n",
    "        newX = self.x_out(newX) * x_mask\n",
    "        assert_correctly_masked(newX, x_mask)\n",
    "\n",
    "        # Process y based on X and E\n",
    "        y = self.y_y(y)\n",
    "        e_y = self.e_y(E)\n",
    "        x_y = self.x_y(X)\n",
    "        new_y = y + x_y + e_y\n",
    "        new_y = self.y_out(new_y)               # bs, dy\n",
    "\n",
    "        return newX, newE, new_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of newX: torch.Size([2, 10, 64])\n",
      "Shape of newE: torch.Size([2, 10, 10, 32])\n",
      "Shape of new_y: torch.Size([2, 16])\n"
     ]
    }
   ],
   "source": [
    "# Example parameters for the NodeEdgeBlock\n",
    "dx = 64\n",
    "de = 32\n",
    "dy = 16\n",
    "n_head = 8\n",
    "bs = 2  # batch size\n",
    "n = 10  # number of nodes\n",
    "\n",
    "# Initialize the NodeEdgeBlock\n",
    "node_edge_block = NodeEdgeBlock(dx, de, dy, n_head)\n",
    "\n",
    "# Create example input tensors\n",
    "X = torch.randn(bs, n, dx)\n",
    "E = torch.randn(bs, n, n, de)\n",
    "y = torch.randn(bs, dy)\n",
    "node_mask = torch.ones(bs, n)  # example mask where all nodes are valid\n",
    "\n",
    "# Forward pass\n",
    "newX, newE, new_y = node_edge_block(X, E, y, node_mask)\n",
    "\n",
    "# Print shapes of the output tensors\n",
    "print(\"Shape of newX:\", newX.shape)\n",
    "print(\"Shape of newE:\", newE.shape)\n",
    "print(\"Shape of new_y:\", new_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XEyTransformerLayer\n",
    "\n",
    "\n",
    "Transformer that updates node, edge and global features. Below formulas only takes the graph topology $\\mathbf E$ as example\n",
    "\n",
    "#### Self Attention\n",
    "\n",
    "$$\\mathbf{X}_{\\text {new }}, \\mathbf{E}_{\\text {new}}, \\mathbf{y}_{\\text {new}}=\\operatorname{SelfAttn}\\left(\\mathbf{X}, \\mathbf{E}, \\mathbf{y}, \\text{node}_\\text{mask}\\right)$$\n",
    "\n",
    "\n",
    "#### Residual and Layer Normalization \n",
    "\n",
    "\\begin{gathered}\n",
    "\\mathbf{E}_{\\text {residual }}=\\mathbf{E}+\\operatorname{Dropout}\\left(\\mathbf{E}_{\\text {new }}\\right) \\\\\n",
    "\\mathbf{E}=\\operatorname{LayerNorm}\\left(\\mathbf{E}_{\\text {residual }}\\right)\n",
    "\\end{gathered}\n",
    "\n",
    "\n",
    "####  Feed-Forward Layer \n",
    "\n",
    "\\begin{gathered}\n",
    "\\mathbf{E}_{\\mathrm{ff}}=\\operatorname{Dropout}\\left(\\operatorname{\\sigma}\\left(\\mathbf{E} \\mathbf{W}_{E 1}+\\mathbf{b}_{E 1}\\right)\\right) \\\\\n",
    "\\mathbf{E}_{\\mathrm{ff}}=\\mathbf{E}_{\\mathrm{ff}} \\mathbf{W}_{E 2}+\\mathbf{b}_{E 2} \\\\\n",
    "\\mathbf{E}_{\\mathrm{ff}}=\\operatorname{Dropout}\\left(\\mathbf{E}_{\\mathrm{ff}}\\right) \\\\\n",
    "\\mathbf{E}=\\mathrm{LayerNorm}\\left(\\mathbf{E}+\\mathbf{E}_{\\mathrm{ff}}\\right)\n",
    "\\end{gathered}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XEyTransformerLayer(nn.Module):\n",
    "    \"\"\" Transformer that updates node, edge and global features\n",
    "        d_x: node features\n",
    "        d_e: edge features\n",
    "        dz : global features\n",
    "        n_head: the number of heads in the multi_head_attention\n",
    "        dim_feedforward: the dimension of the feedforward network model after self-attention\n",
    "        dropout: dropout probablility. 0 to disable\n",
    "        layer_norm_eps: eps value in layer normalizations.\n",
    "    \"\"\"\n",
    "    def __init__(self, dx: int, de: int, dy: int, n_head: int, dim_ffX: int = 2048,\n",
    "                 dim_ffE: int = 128, dim_ffy: int = 2048, dropout: float = 0.1,\n",
    "                 layer_norm_eps: float = 1e-5, device=None, dtype=None) -> None:\n",
    "        kw = {'device': device, 'dtype': dtype}\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attn = NodeEdgeBlock(dx, de, dy, n_head, **kw)\n",
    "\n",
    "        self.linX1 = Linear(dx, dim_ffX, **kw)\n",
    "        self.linX2 = Linear(dim_ffX, dx, **kw)\n",
    "        self.normX1 = LayerNorm(dx, eps=layer_norm_eps, **kw)\n",
    "        self.normX2 = LayerNorm(dx, eps=layer_norm_eps, **kw)\n",
    "        self.dropoutX1 = Dropout(dropout)\n",
    "        self.dropoutX2 = Dropout(dropout)\n",
    "        self.dropoutX3 = Dropout(dropout)\n",
    "\n",
    "        self.linE1 = Linear(de, dim_ffE, **kw)\n",
    "        self.linE2 = Linear(dim_ffE, de, **kw)\n",
    "        self.normE1 = LayerNorm(de, eps=layer_norm_eps, **kw)\n",
    "        self.normE2 = LayerNorm(de, eps=layer_norm_eps, **kw)\n",
    "        self.dropoutE1 = Dropout(dropout)\n",
    "        self.dropoutE2 = Dropout(dropout)\n",
    "        self.dropoutE3 = Dropout(dropout)\n",
    "\n",
    "        self.lin_y1 = Linear(dy, dim_ffy, **kw)\n",
    "        self.lin_y2 = Linear(dim_ffy, dy, **kw)\n",
    "        self.norm_y1 = LayerNorm(dy, eps=layer_norm_eps, **kw)\n",
    "        self.norm_y2 = LayerNorm(dy, eps=layer_norm_eps, **kw)\n",
    "        self.dropout_y1 = Dropout(dropout)\n",
    "        self.dropout_y2 = Dropout(dropout)\n",
    "        self.dropout_y3 = Dropout(dropout)\n",
    "\n",
    "        self.activation = F.relu\n",
    "\n",
    "    def forward(self, X: Tensor, E: Tensor, y, node_mask: Tensor):\n",
    "        \"\"\" Pass the input through the encoder layer.\n",
    "            X: (bs, n, d)\n",
    "            E: (bs, n, n, d)\n",
    "            y: (bs, dy)\n",
    "            node_mask: (bs, n) Mask for the src keys per batch (optional)\n",
    "            Output: newX, newE, new_y with the same shape.\n",
    "        \"\"\"\n",
    "        newX, newE, new_y = self.self_attn(X, E, y, node_mask=node_mask)\n",
    "\n",
    "        newX_d = self.dropoutX1(newX)\n",
    "        X = self.normX1(X + newX_d)\n",
    "\n",
    "        newE_d = self.dropoutE1(newE)\n",
    "        E = self.normE1(E + newE_d)\n",
    "\n",
    "        new_y_d = self.dropout_y1(new_y)\n",
    "        y = self.norm_y1(y + new_y_d)\n",
    "\n",
    "        ff_outputX = self.linX2(self.dropoutX2(self.activation(self.linX1(X))))\n",
    "        ff_outputX = self.dropoutX3(ff_outputX)\n",
    "        X = self.normX2(X + ff_outputX)\n",
    "\n",
    "        ff_outputE = self.linE2(self.dropoutE2(self.activation(self.linE1(E))))\n",
    "        ff_outputE = self.dropoutE3(ff_outputE)\n",
    "        E = self.normE2(E + ff_outputE)\n",
    "\n",
    "        ff_output_y = self.lin_y2(self.dropout_y2(self.activation(self.lin_y1(y))))\n",
    "        ff_output_y = self.dropout_y3(ff_output_y)\n",
    "        y = self.norm_y2(y + ff_output_y)\n",
    "\n",
    "        return X, E, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of newX: torch.Size([2, 10, 64])\n",
      "Shape of newE: torch.Size([2, 10, 10, 32])\n",
      "Shape of new_y: torch.Size([2, 16])\n"
     ]
    }
   ],
   "source": [
    "# Example parameters for the XEyTransformerLayer\n",
    "dx = 64\n",
    "de = 32\n",
    "dy = 16\n",
    "n_head = 8\n",
    "bs = 2  # batch size\n",
    "n = 10  # number of nodes\n",
    "\n",
    "# Initialize the XEyTransformerLayer\n",
    "xey_transformer_layer = XEyTransformerLayer(dx, de, dy, n_head)\n",
    "\n",
    "# Create example input tensors\n",
    "X = torch.randn(bs, n, dx)\n",
    "E = torch.randn(bs, n, n, de)\n",
    "y = torch.randn(bs, dy)\n",
    "node_mask = torch.ones(bs, n)  # example mask where all nodes are valid\n",
    "\n",
    "# Forward pass\n",
    "newX, newE, new_y = xey_transformer_layer(X, E, y, node_mask)\n",
    "\n",
    "# Print shapes of the output tensors\n",
    "print(\"Shape of newX:\", newX.shape)\n",
    "print(\"Shape of newE:\", newE.shape)\n",
    "print(\"Shape of new_y:\", new_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GraphTransformer \n",
    "\n",
    "The `GraphTransformer` processes node $\\mathbf{X}$, edge $\\mathbf{E}$, and global $\\mathbf{y}$ features through MLPs, applies multiple transformer layers to update the features, and then processes the updated features through output MLPs before returning the final results with symmetric edge features and masked nodes.\n",
    "\n",
    "1. **Initial Feature Processing:**\n",
    "\n",
    "   \\begin{aligned}\n",
    "   &\\mathbf{X}_{\\text{in}} = \\sigma(\\mathbf{W}_{1X} \\sigma(\\mathbf{W}_{0X} \\mathbf{X} + \\mathbf{b}_{0X}) + \\mathbf{b}_{1X}) \\\\\n",
    "   &\\mathbf{E}_{\\text{in}} = \\frac{1}{2} (\\sigma(\\mathbf{W}_{1E} \\sigma(\\mathbf{W}_{0E} \\mathbf{E} + \\mathbf{b}_{0E}) + \\mathbf{b}_{1E}) + \\sigma(\\mathbf{W}_{1E} \\sigma(\\mathbf{W}_{0E} \\mathbf{E}^T + \\mathbf{b}_{0E}) + \\mathbf{b}_{1E})) \\\\\n",
    "   &\\mathbf{y}_{\\text{in}} = \\sigma(\\mathbf{W}_{1y} \\sigma(\\mathbf{W}_{0y} \\mathbf{y} + \\mathbf{b}_{0y}) + \\mathbf{b}_{1y}) \\\\\n",
    "   \\end{aligned}\n",
    "\n",
    "\n",
    "2. **Transformer Layers:**\n",
    "   For each transformer layer $i$:\n",
    "\n",
    "   $$\\mathbf{X}, \\mathbf{E}, \\mathbf{y} = \\text{XEyTransformerLayer}[i](\\mathbf{X}, \\mathbf{E}, \\mathbf{y}, \\text{node}_{\\textrm{mask}})$$\n",
    "\n",
    "\n",
    "3. **Final Feature Processing:**\n",
    "\n",
    "\n",
    "   \\begin{aligned}\n",
    "   &\\mathbf{X}_{\\text{out}} = \\sigma(\\mathbf{W}_{3X} \\sigma(\\mathbf{W}_{2X} \\mathbf{X} + \\mathbf{b}_{2X}) + \\mathbf{b}_{3X}) + \\mathbf{X} \\\\\n",
    "   &\\mathbf{E}_{\\text{out}} = \\sigma(\\mathbf{W}_{3E} \\sigma(\\mathbf{W}_{2E} \\mathbf{E} + \\mathbf{b}_{2E}) + \\mathbf{b}_{3E}) \\odot \\text{diag}_{\\text{mask}} + \\mathbf{E} \\odot \\text{diag}_{\\text{mask}} \\\\\n",
    "   &\\mathbf{y}_{\\text{out}} = \\sigma(\\mathbf{W}_{3y} \\sigma(\\mathbf{W}_{2y} \\mathbf{y} + \\mathbf{b}_{2y}) + \\mathbf{b}_{3y}) + \\mathbf{y} \\\\\n",
    "   \\end{aligned}\n",
    "\n",
    "\n",
    "4. **Symmetrizing Edge Features:**\n",
    "\n",
    "   $$\\mathbf{E}_{\\text{out}} = \\frac{1}{2} (\\mathbf{E}_{\\text{out}} + \\mathbf{E}_{\\text{out}}^T)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GraphTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    n_layers : int -- number of layers\n",
    "    dims : dict -- contains dimensions for each feature type\n",
    "    \"\"\"\n",
    "    def __init__(self, n_layers: int, input_dims: dict, hidden_mlp_dims: dict, hidden_dims: dict,\n",
    "                 output_dims: dict, act_fn_in: nn.ReLU(), act_fn_out: nn.ReLU()):\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.out_dim_X = output_dims['X']\n",
    "        self.out_dim_E = output_dims['E']\n",
    "        self.out_dim_y = output_dims['y']\n",
    "\n",
    "        self.mlp_in_X = nn.Sequential(nn.Linear(input_dims['X'], hidden_mlp_dims['X']), act_fn_in,\n",
    "                                      nn.Linear(hidden_mlp_dims['X'], hidden_dims['dx']), act_fn_in)\n",
    "\n",
    "        self.mlp_in_E = nn.Sequential(nn.Linear(input_dims['E'], hidden_mlp_dims['E']), act_fn_in,\n",
    "                                      nn.Linear(hidden_mlp_dims['E'], hidden_dims['de']), act_fn_in)\n",
    "\n",
    "        self.mlp_in_y = nn.Sequential(nn.Linear(input_dims['y'], hidden_mlp_dims['y']), act_fn_in,\n",
    "                                      nn.Linear(hidden_mlp_dims['y'], hidden_dims['dy']), act_fn_in)\n",
    "\n",
    "        self.tf_layers = nn.ModuleList([XEyTransformerLayer(dx=hidden_dims['dx'],\n",
    "                                                            de=hidden_dims['de'],\n",
    "                                                            dy=hidden_dims['dy'],\n",
    "                                                            n_head=hidden_dims['n_head'],\n",
    "                                                            dim_ffX=hidden_dims['dim_ffX'],\n",
    "                                                            dim_ffE=hidden_dims['dim_ffE'])\n",
    "                                        for i in range(n_layers)])\n",
    "\n",
    "        self.mlp_out_X = nn.Sequential(nn.Linear(hidden_dims['dx'], hidden_mlp_dims['X']), act_fn_out,\n",
    "                                       nn.Linear(hidden_mlp_dims['X'], output_dims['X']))\n",
    "\n",
    "        self.mlp_out_E = nn.Sequential(nn.Linear(hidden_dims['de'], hidden_mlp_dims['E']), act_fn_out,\n",
    "                                       nn.Linear(hidden_mlp_dims['E'], output_dims['E']))\n",
    "\n",
    "        self.mlp_out_y = nn.Sequential(nn.Linear(hidden_dims['dy'], hidden_mlp_dims['y']), act_fn_out,\n",
    "                                       nn.Linear(hidden_mlp_dims['y'], output_dims['y']))\n",
    "\n",
    "    def forward(self, X, E, y, node_mask):\n",
    "        bs, n = X.shape[0], X.shape[1]\n",
    "\n",
    "        diag_mask = torch.eye(n)\n",
    "        diag_mask = ~diag_mask.type_as(E).bool()\n",
    "        diag_mask = diag_mask.unsqueeze(0).unsqueeze(-1).expand(bs, -1, -1, -1)\n",
    "\n",
    "        X_to_out = X[..., :self.out_dim_X]\n",
    "        E_to_out = E[..., :self.out_dim_E]\n",
    "        y_to_out = y[..., :self.out_dim_y]\n",
    "\n",
    "        new_E = self.mlp_in_E(E)\n",
    "        new_E = (new_E + new_E.transpose(1, 2)) / 2\n",
    "        # logging.debug(f\"X shape: {X.shape}\")\n",
    "        after_in = PlaceHolder(X=self.mlp_in_X(X), E=new_E, y=self.mlp_in_y(y)).mask(node_mask)\n",
    "        # logging.debug(f\"after_in.X shape: {after_in.X.shape}\")\n",
    "        X, E, y = after_in.X, after_in.E, after_in.y\n",
    "\n",
    "        for layer in self.tf_layers:\n",
    "            X, E, y = layer(X, E, y, node_mask)\n",
    "\n",
    "        X = self.mlp_out_X(X)\n",
    "        E = self.mlp_out_E(E)\n",
    "        y = self.mlp_out_y(y)\n",
    "\n",
    "        X = (X + X_to_out)\n",
    "        E = (E + E_to_out) * diag_mask\n",
    "        y = y + y_to_out\n",
    "\n",
    "        E = 1/2 * (E + torch.transpose(E, 1, 2))\n",
    "\n",
    "        return PlaceHolder(X=X, E=E, y=y).mask(node_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of output.X: torch.Size([2, 10, 64])\n",
      "Shape of output.E: torch.Size([2, 10, 10, 32])\n",
      "Shape of output.y: torch.Size([2, 16])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Example parameters\n",
    "n_layers = 3\n",
    "input_dims = {'X': 64, 'E': 32, 'y': 16}\n",
    "hidden_mlp_dims = {'X': 128, 'E': 64, 'y': 32}\n",
    "hidden_dims = {'dx': 64, 'de': 32, 'dy': 16, 'n_head': 8, 'dim_ffX': 256, 'dim_ffE': 128}\n",
    "output_dims = {'X': 64, 'E': 32, 'y': 16}\n",
    "\n",
    "# Initialize the GraphTransformer\n",
    "graph_transformer = GraphTransformer(\n",
    "    n_layers=n_layers,\n",
    "    input_dims=input_dims,\n",
    "    hidden_mlp_dims=hidden_mlp_dims,\n",
    "    hidden_dims=hidden_dims,\n",
    "    output_dims=output_dims,\n",
    "    act_fn_in=nn.ReLU(),\n",
    "    act_fn_out=nn.ReLU()\n",
    ")\n",
    "\n",
    "# Create example input tensors\n",
    "bs = 2  # batch size\n",
    "n = 10  # number of nodes\n",
    "X = torch.randn(bs, n, input_dims['X'])\n",
    "E = torch.randn(bs, n, n, input_dims['E'])\n",
    "y = torch.randn(bs, input_dims['y'])\n",
    "node_mask = torch.ones(bs, n)  # example mask where all nodes are valid\n",
    "\n",
    "# Forward pass\n",
    "output = graph_transformer(X, E, y, node_mask)\n",
    "\n",
    "# Print shapes of the output tensors\n",
    "print(\"Shape of output.X:\", output.X.shape)\n",
    "print(\"Shape of output.E:\", output.E.shape)\n",
    "print(\"Shape of output.y:\", output.y.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gad_v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
